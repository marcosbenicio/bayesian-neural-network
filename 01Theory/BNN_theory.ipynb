{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Layer, Dropout, BatchNormalization, LeakyReLU, Lambda\n",
    "from keras.losses import Loss, mse, MeanSquaredError\n",
    "from keras.optimizers import Optimizer, Adam\n",
    "from keras.metrics import Mean\n",
    "from keras.utils import to_categorical, plot_model, load_img, img_to_array\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Neural Network (BNN) - Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Probabilities and Bayes Rule\n",
    "\n",
    "In Bayesian statistics probability is defined in terms of degree of belief. The he more likely an outcome or a certain value of a parameter is, the higher the degree of belief in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ordinary Probability**\n",
    "\n",
    "In ordinary probability theory, we consider events and their associated probabilities, which describe the certainty or possibility of these events occurring in a random experiment.\n",
    "\n",
    "A random experiment is a procedure or process that yields an outcome from a given sample space, where the sample space is the set of all possible outcomes. Each outcome is considered an elementary event, and a collection of elementary events forms an event. The probability of an event is a measure of the certainty that the event will occur, with this measure being a number between 0 and 1.\n",
    "\n",
    "A fundamental concept in probability theory is the law of large numbers, which asserts that as the number of trials in a random experiment increases indefinitely, the relative frequency of any given event will converge to the true probability of that event. Mathematically, this can be expressed as:\n",
    "\n",
    "\n",
    "$$\\lim_{n \\to \\infty} \\frac{n(A)}{n} = P(A)$$\n",
    "\n",
    "where $n$ is the total number of trials, $n(A)$ is the number of times event $A$ occurs, and $P(A)$ is the true probability of event $A$. This law estimate probabilities through the relative frequencies observed in empirical settings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Addition Law for Probability**\n",
    "\n",
    "Consider two mutually exclusive events $A_1$ and $A_2$ associated with the outcomes of a random experiment, and let $A = A_1 \\bigcup A_2$ be the union of the two events. If events $A_1$ and $A_2$ are mutually exclusive, by definition, they cannot occur simultaneously, and we denote this as $A_1 \\bot A_2$. This implies that the intersection of these two events must be $A_1 \\bigcap A_2 = \\emptyset$. If $A$ occurs in a trial, it means that either event $A_1$ has occurred, or event $A_2$ has occurred, but not both since $A_1$ and $A_2$ are mutually exclusive.\n",
    "\n",
    "The union of the events $A_1 \\bigcup A_2$ includes all the outcomes of both events, without any overlap since they are mutually exclusive. So, when counting the number of outcomes in $A$, we are essentially counting the number of outcomes in $A_1$ and $A_2$ separately and then adding them together. Therefore, we can write:\n",
    "\n",
    "$$\\frac{n(A)}{n} = \\frac{n(A_1)}{n} + \\frac{n(A_2)}{n}$$\n",
    "\n",
    "where $n$ is the total number of trials in the experiment, and $n(A)$, $n(A_1)$, $n(A_2)$ are the total number of trials leading to events $A$, $A_1$, and $A_2$, respectively.\n",
    "\n",
    "For a sufficiently large number of trials $n$, the relative frequencies $\\frac{n(A)}{n}$, $\\frac{n(A_1)}{n}$, $\\frac{n(A_2)}{n}$ will coincide with the corresponding probabilities $P(A)$, $P(A_1)$, $P(A_2)$. We get:\n",
    "\n",
    "$$P(A) = P(A_1) + P(A_2)$$\n",
    "\n",
    "Similarly, if events $A_1$, $A_2$, and $A_3$ are mutually exclusive, it means that no two of these events can occur simultaneously. This implies that their pairwise intersections are empty: $A_1 \\bigcap A_2 = \\emptyset$, $A_2 \\bigcap A_3 = \\emptyset$, and $A_1 \\bigcap A_3 = \\emptyset$. As a result, the union of events $A_1$ and $A_2$ is also mutually exclusive with $A_3$. For the probability, this can be expressed as:\n",
    "\n",
    "$$P(A_1 \\bigcup A_2 \\bigcup A_3) = P(A_1 \\bigcup A_2 \\bot A_3) = P(A_1 \\bigcup A_2) + P(A_3)$$\n",
    "\n",
    "Since $A_1$ and $A_2$ are mutually exclusive, we have:\n",
    "\n",
    "$$P(A_1 \\bigcup A_2 \\bigcup A_3) =  P(A_1) + P(A_2) + P(A_3)$$\n",
    "\n",
    "More generally, given $n$ mutually exclusive events $A_1, A_2, \\cdots, A_n$, we have the formula:\n",
    "\n",
    "$$P(\\bigcup_{k=1}^{n}A_{k}) = \\sum_{k=1}^{n} P(A_{k}) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conditional Probability**\n",
    "\n",
    "Conditional probability helps us understand the relationship between two events, $A$ and $B$. In particular, it describes the probability of event $A$ given that event $B$ has already occurred. The conditional probability of event $A$ given event $B$ is defined as:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}.$$\n",
    "\n",
    "Here, $P(A \\cap B)$ represents the joint probability, which signifies the likelihood of two or more events occurring simultaneously. For events $A$ and $B$, the joint probability is denoted by $P(A \\cap B)$, indicating the probability of both events $A$ and $B$ happening together.\n",
    "\n",
    "Suppose we conduct a random experiment with a finite number of outcomes, all having equal probability. Let $N$ represent the total number of trials, $N(B)$ be the number of trials resulting in event $B$, and $N(A \\cap B)$ be the number of trials where both $A$ and $B$ occur together. In this case, the probability of $B$ and the joint probability $A \\cap B$ can be expressed as:\n",
    "\n",
    "$$P(B) = \\frac{N(B)}{N}, ~~~~~~P(A \\cap B) = \\frac{N(A \\cap B)}{N},$$\n",
    "\n",
    "Using these expressions, we can rewrite the conditional probability of $A$ given $B$ as:\n",
    "\n",
    "$$P(A|B) = \\frac{N(A \\cap B)}{N(B)}.$$\n",
    "\n",
    "This formulation helps clarify the meaning of conditional probability. Essentially, it calculates the proportion of trials where both $A$ and $B$ occur (joint probability) out of the trials where event $B$ occurs.\n",
    "\n",
    "\n",
    "- If $A$ and $B$ are mutually exclusive events, then $A \\cap B = \\emptyset$ and $P(A|B) = 0$.\n",
    "- If $A$ implies in  $B$ ($B$ is subset of $A$: $A \\subset B$ ), then $P(A|B) = 1$.\n",
    "- If $A_1, \\cdots, A_n$ are mutually exclusive events, with union $A = \\bigcup_{k=1}^{n}A_{k}$, then the addition law for conditional probability is\n",
    "  $P(A|B) = \\sum_k P(A_k|B)$.\n",
    "\n",
    "$\\textbf{proof}:$\n",
    "  \n",
    "$$A \\cap B = \\bigcup_{k=1}^{n}(A_{k}\\cap B) $$\n",
    "that gives the union of each intersection of $A_k \\cap B$ for each $k = 1,2,...n$. By the addition law : \n",
    "\n",
    "$$P(\\bigcup_{k=1}^{n}(A_{k}\\cap B)) = \\sum_{k=1}^{n} P(A_{k}\\cap B) $$\n",
    "\n",
    "and dividing  by $P(B)$ we get:\n",
    "\n",
    "$$\\frac{P(\\bigcup_{k=1}^{n}(A_{k}\\cap B))}{P(B)} = \\sum_{k=1}^{n} \\frac{P(A_{k}\\cap B)}{P(B)} $$\n",
    "\n",
    "then\n",
    "$$P(A|B) = \\sum_{k=1}^{n} P(A_k|B) ~~~~\\square$$\n",
    "\n",
    "**Conditional probability as an intermediate step**\n",
    "\n",
    "Suppose we have a complete set of mutually exclusive and exhaustive events $B_1, \\cdots, B_n$, meaning only one of these events can occur at a time, and their union covers the entire sample space. We can find the ordinary probability of event $A$ occurring using the total probability formula:\n",
    "\n",
    "$$P(A) = \\sum_k P(A|B_k)P(B_k)$$\n",
    "\n",
    "To prove this, first let's define the concept of mutually exclusive and exhaustive events. Mutually exclusive events are events that cannot occur simultaneously, meaning their intersection is indeed an empty set. On the other hand, exhaustive events are events that, when considered together, cover the entire sample space. When a set of events is both mutually exclusive and exhaustive, it means that they cover all possible outcomes without overlapping.\n",
    "\n",
    "To better understand mutually exclusive and exhaustive events, consider the example of rolling a fair six-sided die. The sample space for this experiment is the set of outcomes $\\{1, 2, 3, 4, 5, 6\\}$. We can define the events as follows:\n",
    "\n",
    "- Event $A_1$: The die shows an odd number (outcomes: $\\{1, 3, 5\\}$)\n",
    "- Event $A_2$: The die shows an even number (outcomes: $\\{2, 4, 6\\}$)\n",
    "\n",
    "The events $A_1$ and $A_2$ are mutually exclusive, because no outcome can be both odd and even simultaneously, meaning the intersection of $A_1$ and $A_2$ is an empty set. Furthermore, these events are exhaustive, because together they cover the entire sample space (every possible outcome is either odd or even). Thus, the set {$A_1$, $A_2$} is both mutually exclusive and exhaustive.\n",
    "\n",
    "$\\textbf{proof}:$\n",
    "\n",
    "\n",
    "Consider $\\Omega$ as the sample space. If $B_1, \\cdots, B_n$ are all the possible mutually exclusive and exhaustive events. If their union covers the entire sample space, then:\n",
    "\n",
    "$$\\bigcup_k B_k= \\Omega$$\n",
    "\n",
    "Consequently, event $A$ can be expressed as the union of its intersections with each of the mutually exclusive events $B_k$:\n",
    "\n",
    "$$A = \\bigcup_k (A \\cap B_k)$$\n",
    "\n",
    "Since the events $A \\cap B_k$ are mutually exclusive, we can apply the addition rule of probability:\n",
    "$$P(A) = P(\\bigcup_k (A \\cap B_k) = \\sum_k P(A \\cap B_k)$$\n",
    "\n",
    "Now we can rewrite the probability of the intersection using conditional probability:\n",
    "\n",
    "$$P(A) = P(\\bigcup_k (A \\cap B_k)) =  \\sum_k P(A \\cap B_k) = \\sum_k \\frac{P(A \\cap B_k)}{P(B_k)} P(B_k) = \\sum_k  P(A |B_k)P(B_k)~~~~ \\square$$\n",
    "\n",
    "This proof demonstrates how we can find the probability of an event $A$ occurring by considering its relationship with a set of mutually exclusive and exhaustive events $B_1, \\cdots, B_n$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bayes Rule**\n",
    "\n",
    "Given two events A and B, Bayes' theorem relates the conditional probability of A given B ($P(A|B)$) to the conditional probability of B given A ($P(B|A)$), along with the individual probabilities of A ($P(A)$) and B ($P(B)$):\n",
    "\n",
    "​$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} = \\frac{P(B|A) \\cdot P(A)}{\\sum_k P(B|A_k)P(A_k)} $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $P(A|B)$: is the **posterior probability**. It represents the probability of event A happening, given that event B has occurred. This is what we're trying to find using Bayes' theorem. It reflects our updated belief about event A after taking into account the new information provided by event B.\n",
    "\n",
    "- $P(B|A)$: is the **likelihood**. It represents the probability of event B happening, given that event A has occurred. This is often a known value or can be estimated from available data. It tells us how likely it is to observe event B when event A is true.\n",
    "\n",
    "- $P(A)$: is the **prior probability**. It represents the probability of event A happening before taking into account any new information from event B. This is our initial belief about event A and can be based on previous data, expert opinion, or assumptions.\n",
    "\n",
    "- $P(B)$: is the **marginal probability or evidence**. It represents the overall probability of event B happening, regardless of whether event A happens or not. This value can be calculated using the law of total probability, which takes into account both the probabilities of event A and its complement, event $\\neg A$ (i.e., event A not happening). Specifically, if you have a finite set of mutually exclusive and exhaustive events $A_1, \\cdots, A_n$, then the probability of event B can be expressed as:\n",
    "  $$P(B) = \\sum_k P(B|A_k)P(A_k)$$\n",
    "  - **Mutually exclusive:** The events $A_k$ do not occur simultaneously. For any pair of events $A_i$ and $A_j$, if $i \\neq j$, then $P(A_i \\cap A_j) = 0$.\n",
    "  - **Exhaustive:** The union of all events $A_k$ covers the entire sample space, meaning that at least one of the events $A_k$ must occur. Mathematically, $\\bigcup_k A_k = \\Omega$, where $\\Omega$ is the sample space.\n",
    "\n",
    "The main idea behind Bayes' theorem is to update our belief about event A (the prior probability) using the new information provided by event B (the likelihood). The result is the posterior probability, which reflects our revised belief about event A after considering the occurrence of event B. It's important to note that the Bayes' rule itself doesn't make any assumptions about the relationship between A and B; it's a general formula applicable to any pair of events.\n",
    "\n",
    "$\\textbf{proof}:$\n",
    "\n",
    "For event A given event B, the conditional probability is defined as:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\bigcap B)}{P(B)}$$\n",
    "\n",
    "Likewise, the conditional probability of event B given event A is expressed as:\n",
    "$$P(B|A) = \\frac{P(A \\bigcap B)}{P(A)}$$\n",
    "\n",
    "Our objective is to derive Bayes' theorem, which connects $P(A|B)$ and $P(B|A)$. To achieve this, we first isolate $P(A \\cap B)$ in the second equation::\n",
    "$$P(A \\bigcap B) = P(B|A) \\cdot P(A)$$\n",
    "\n",
    "Next, substitute this expression for $P(A \\cap B)$ into the first equation:\n",
    "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}~~~~\\square$$\n",
    "\n",
    "This is Bayes' theorem. It links the conditional probability of A given B with the conditional probability of B given A, factoring in the individual probabilities of A and B. The product $P(B|A)P(A)$ represents the joint probability of events A and B occurring, which is $P(A \\cap B)$. The connection between this product and the total probability formula shows how the probability of A given B is related to the joint probability of A and B, as well as the probabilities of A and B given different events in the set $\\{A_1, \\cdots, A_n\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler (KL) Divergence (Relative Entropy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kullback-Leibler (KL) divergence is a measure that quantifies the difference or \"distance\" between two probability distributions. It is a powerful tool for comparing and contrasting distributions, which is particularly useful in areas such as information theory, machine learning, and statistical inference. Given two probability distributions, $p(x)$ and $q(x)$, the KL divergence can be defined using the following formula:\n",
    "\n",
    "$$D_{KL}(p||q) =\\mathbb{E}\\bigg[\\log{\\bigg(\\frac{p(x)}{q(x)}\\bigg)}\\bigg] $$\n",
    "\n",
    "In the discrete case, the KL divergence is computed as:\n",
    "\n",
    "$$D_{KL}(p||q) = \\sum_x p(x)\\log{\\bigg(\\frac{p(x)}{q(x)}\\bigg)}$$\n",
    "\n",
    "For continuous probability distributions, the KL divergence is given by the integral:\n",
    "\n",
    "$$D_{KL}(p||q) = \\int_x p(x)\\log{\\bigg(\\frac{p(x)}{q(x)}\\bigg)}$$\n",
    "\n",
    "It is important to note that the KL divergence is not symmetric, meaning that $D_{KL}(p||q) \\neq D_{KL}(q||p)$. The KL divergence can be interpreted as the expected logarithmic difference between the two probability distributions, with the expectation taken over the values of $x$ according to the distribution $p(x)$. In essence, the KL divergence quantifies the amount of information lost when we approximate the true distribution $p(x)$ using the distribution $q(x)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1 - Discrete**\n",
    "\n",
    "Consider the set $w \\in {0:Bad, 1:Good}$ representing the weather, and suppose that the weather being good or bad follows a Bernoulli distribution $w \\sim \\textbf{Bernoulli}(\\theta)$. In a hypothetical scenario, two people are arguing about the chance of the weather being good. The first person says that $\\theta_A = 0.8$, and the second person says that $\\theta_B = 0.7$. How far apart are their opinions (distributions)?\n",
    "\n",
    "For a Bernoulli distribution, it can be thought of as a model for the set of possible outcomes of any single experiment that asks a yes–no question. The Bernoulli distribution can be written as\n",
    "\n",
    "$$\\textbf{Bernoulli}(w,\\theta) = \\theta^w(1-\\theta)^{1-w}$$\n",
    "\n",
    "and the KL divergence as\n",
    "\n",
    "$$D_{KL}(p||q) = \\sum^{1}_{w=0} p(w)\\log{\\bigg(\\frac{p(w)}{q(w)}\\bigg)}$$\n",
    "\n",
    "where the probabilities are $p(w) = \\theta_A^w(1-\\theta_A)^{1-w}$ and $q(w) = \\theta_B^w(1-\\theta_B)^{1-w}$ . substituting in the Kl divergence:\n",
    "\n",
    "$$D_{KL}(p||q) = \\sum^{1}_{w=0} \\theta_A^w(1-\\theta_A)^{1-w}\\log{\\bigg(\\frac{\\theta_A^w(1-\\theta_A)^{1-w}}{\\theta_B^w(1-\\theta_B)^{1-w}}\\bigg)}$$\n",
    "\n",
    "Now, let's evaluate the sum for $w=0$ and $w=1$:\n",
    "\n",
    "$$D_{KL}(p||q) = (1-\\theta_A)\\log{\\bigg(\\frac{1-\\theta_A}{1-\\theta_B}\\bigg)} + \\theta_A\\log{\\bigg(\\frac{\\theta_A}{\\theta_B}\\bigg)}$$\n",
    "\n",
    "Now, we can plug in the values $\\theta_A = 0.8$ and $\\theta_B = 0.7$ to compute the KL divergence:\n",
    "\n",
    "$$D_{KL}(p||q) \\approx 0.02573$$\n",
    "\n",
    "This value represents the distance between the two opinions (distributions) held by the two people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D(p||q)= 0.025732100009918213\n"
     ]
    }
   ],
   "source": [
    "weather_A = tfp.distributions.Bernoulli(probs = 0.8)\n",
    "weather_B = tfp.distributions.Bernoulli(probs = 0.7)\n",
    "\n",
    "print('D(p||q)=', float(tfp.distributions.kl_divergence(weather_A, weather_B)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2 - Continous**\n",
    "\n",
    "Suppose we're studying the height of adult men in two countries: Country A and Country B.\n",
    "The height of adult men in Country A follows a Gaussian distribution with mean 175 cm and standard deviation 10 cm. The height of adult men in Country B follows a Gaussian distribution with mean 170 cm and standard deviation 15 cm.\n",
    "\n",
    "We can use the Kullback-Leibler (KL) divergence to measure the difference between these two distributions. \n",
    "\n",
    "First, recall that the KL divergence between two Gaussian distributions $p(x)\\sim \\mathcal{N}(\\mu_p, \\sigma_p )$ and $q(x)\\sim \\mathcal{N}(\\mu_q,\\sigma_q )$ is \n",
    "\n",
    "$$D_{KL}(p||q) = \\int_x p(x)\\log{\\bigg(\\frac{p(x)}{q(x)}\\bigg)}$$\n",
    "\n",
    "We can substitute their probability density functions into this integral and simplify the result. A probability density function $f(x) \\sim \\mathcal{N}(\\mu,\\sigma )$ for a Gaussian distribution with mean $\\mu$ and standard deviation $\\sigma$ is given by\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp\\bigg(-\\frac{(x-\\mu)^2}{2 \\sigma^2}\\bigg)$$\n",
    "\n",
    "Substituting the expressions for $p(x) \\sim \\mathcal{N}(\\mu_p,\\sigma_p )$ and $q(x)\\sim \\mathcal{N}(\\mu_q,\\sigma_q )$  into the KL divergence integral and simplifying yields the formula:\n",
    "\n",
    "$$D_{KL}(p||q) = \\log\\left(\\frac{\\sigma_q}{\\sigma_p}\\right) + \\frac{\\sigma_p^2 + (\\mu_p - \\mu_q)^2}{2\\sigma_q^2} - \\frac{1}{2}$$\n",
    "\n",
    "Here, $p(x) \\sim \\mathcal{N}(175.3,7.1 )$ and $q(x)\\sim \\mathcal{N}(170.8,6.3)$. Substituting these values into the formula for the KL divergence gives:\n",
    "\n",
    "$$D_{KL}(P||Q) = \\log\\left(\\frac{6.3}{7.1}\\right) + \\frac{(7.1)^2 +(175.3−170.8)^2 \n",
    "}{2(6.3))^2} - \\frac{1}{2} \\approx 0.183$$\n",
    "\n",
    "$D_{KL}(P||Q) = 0.183$ indicate the amount of information lost when the distribution for Country B is used to approximate the distribution for Country A. This gives us a notion of how distance this two distribution are from each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D(p||q)= 0.18324309587478638\n"
     ]
    }
   ],
   "source": [
    "mu_p, sigma_p = 175, 10\n",
    "mu_q, sigma_q = 170, 15\n",
    "\n",
    "# distributions\n",
    "dist_p = tfp.distributions.Normal(loc=mu_p, scale=sigma_p)\n",
    "dist_q = tfp.distributions.Normal(loc=mu_q, scale=sigma_q)\n",
    "\n",
    "# KL divergence\n",
    "kl_divergence = tfp.distributions.kl_divergence(dist_p, dist_q)\n",
    "print('D(p||q)=', float(kl_divergence))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference (VI) and Evidence Lower Bound (ELBO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational inference is a technique used to make predictions and reason about uncertainty in complex probabilistic models by approximating the posterior distribution of latent variables given observed data. Consider a dataset $D$ and latent variables $z$, which are unobserved variables that capture hidden patterns or underlying structure in the data. The goal is to compute the posterior distribution $p(z|D)$, which represents our updated belief about the latent variables $z$ after observing the dataset $D$. To achieve this, we rely on Bayes' rule:\n",
    "\n",
    "​$$p(z|D) = \\frac{p(D|z) \\cdot P(z)}{p(D)} = \\frac{p(D|z) \\cdot p(z)}{\\int p(D|z')p(z')dz'} $$\n",
    "\n",
    "In the continuous case, we use an integral instead of a summation, as the latent variables have a continuous domain rather than a finite set of discrete events. The integral represents a continuous sum over all possible values of the latent variable within its domain.\n",
    "\n",
    "To compute the marginal likelihood, $p(D)$, for continuous latent variables, we need to integrate over the entire domain of the latent variables to account for all possible configurations. The integral in the denominator of Bayes' rule, $\\int p(D|z')P(z') dz'$, serves to sum the probabilities over the entire continuous domain of the latent variable $z$.\n",
    "\n",
    "The challenge in directly using Bayes' rule lies in calculating the marginal probability $p(D) = \\int p(D|z')p(z') dz'$. This integral can be computationally expensive or even intractable for large or high-dimensional latent variable spaces. To address this issue, variational inference approximates the true posterior distribution with a simpler distribution, making the computation more tractable.\n",
    "\n",
    "To tackle this problem, let's consider the continuous KL-divergence to find some distribution $q(z')$ to approximate the distribution for the posterior probabilities $p(z'|D)$. The goal is to find a distribution $q(z')$ that minimizes this divergence, hence approximating the true distribution as closely as possible.\n",
    "\n",
    "$$D_{KL}(q||p) = \\int q(z')\\log{\\bigg(\\frac{q(z')}{p(z'|D)}\\bigg)}dz'$$\n",
    "\n",
    "The usage of $q(z')$ instead of $p(z'|D)$ in the integrand is due to that we do not have direct access to the true posterior distribution. While it might seem like we're \"putting the cart before the horse\" by using the approximating distribution $q(z')$ in the integrand, this approach is justified by the practical benefits it provides in the context of variational inference, providing a more flexible choice for $q(z')$.\n",
    "\n",
    "Substituting the Bayes' rule $p(z'|D) = \\frac{p(D|z')p(z')}{p(D)}$ in the KL divergence:\n",
    "\n",
    "$$D_{KL}(q||p) = \\int q(z')\\log{\\bigg(\\frac{q(z')p(D)}{p(D \\cap z')}\\bigg)}dz'$$\n",
    "\n",
    "We have the information about the joint probability $p(D|z')p(z')$, but not about the marginal $p(D)$. So rearranging the integral in terms of probabilities that we have and don't have:\n",
    "\n",
    "$$D_{KL}(q||p) = \\int q(z')\\log{\\bigg(\\frac{q(z')}{p(D \\cap z')}\\bigg)}dz'+ \\int q(z')\\log{\\bigg( p(D) \\bigg)}dz'$$\n",
    "\n",
    "From the definition of expectation value, if $p(z)$ is a continuous distribution of the random variable $z$ and $g(z)$ is some function of $z$, then $\\mathbb{E}[g(z)] = \\int g(z)p(z)dz$. So, the KL-divergence can be written as a sum of expectation values over the variable $z'$ from the distribution $q(z')$:\n",
    "\n",
    "$$D_{KL}(q||p) = \\mathbb{E}_{z' \\sim q}\\bigg[\\log{\\frac{q(z')}{p(D \\cap z')}}\\bigg]+ \\mathbb{E}_{z' \\sim q}[\\log{p(D)}]$$\n",
    "\n",
    "Since $p(D)$ is not dependent on $z$, the expectation $\\mathbb{E}_{z' \\sim q}[\\log{p(D)}]$ simplifies to $\\log{p(D)}$. Then, we reverse the fraction in the first expectation and add a minus sign in the logarithm:\n",
    "\n",
    "$$D_{KL}(q||p) = -\\mathbb{E}_{z' \\sim q}\\bigg[\\log{\\frac{p(D \\cap z')}{q(z')}}\\bigg]+\\log{p(D)}$$\n",
    "\n",
    "Let's name the first expectation as $\\mathcal{L}(q(z);D) = \\mathbb{E}_{z' \\sim q}\\bigg[\\log{\\frac{p(D|z')p(z')}{q(z')}}\\bigg]$ and write the KL-divergence as:\n",
    "\n",
    "$$D_{KL}(q||p) = -\\mathcal{L}(q(z);D) + \\log{p(D)}$$\n",
    "\n",
    "We know that the KL-divergence must be a non-negative value, as we are interested in measuring the distance between the posterior distribution and the distribution $q(z)$ to approximate the posterior. The term $\\log{p(D)}$ is fixed for a given dataset and also negative (log of values between zero and one). For the term $\\mathcal{L}(q(z);D)$, we expect negative values to balance the negative term $\\log{p(D)}$ when the two distributions are close. The value of $\\mathcal{L}(q(z);D)$ is expected to be smaller than the value of the evidence or marginal probability $\\log{p(D)}$, and because of this, $\\mathcal{L}(q(z);D)$ is called the Evidence Lower Bound (ELBO).\n",
    "\n",
    "- $\\mathcal{L}(q(z);D) = \\log{p(D)}$ if only if $D_{KL}(q||p) = 0$\n",
    "  - **Evidence Lower Bound** (ELBO): $\\mathcal{L}(q(z);D) = \\mathbb{E}_{z' \\sim q}\\bigg[\\log{\\frac{p(D \\cap z')}{q(z')}}\\bigg]$ \n",
    "  - **Evidence** : $\\log{p(D)}$\n",
    "\n",
    "The ELBO can be evaluated because it involves quantities that we do know: the likelihood $p(D|z')$, the prior $p(z')$ and the approximating distribution $q(z')$.\n",
    "\n",
    "Now we arrive at the concept of Variational inference, where $D_{KL}(q||p)$ is a non-negative value when the distributions are not close and close to zero when the two distributions are approximated. So we are approximating our evidence $\\log{p(D)}$ from below, and this is the case where we need to maximize the ELBO term $\\mathcal{L}(q(z);D)$ for a certain family of distributions $Q$. The family $Q$ is a set of candidate distributions, and our goal is to find the best distribution $q^*(z)$ within that family that approximates the true posterior distribution $p(z|D)$:\n",
    "\n",
    "$$q^*(z)  = \\operatorname{argmax}_{q \\in Q}(\\mathcal{L}(q(z);D))$$\n",
    "\n",
    "For this maximization, we rearrange the ELBO in terms of the KL-Divergence:\n",
    "\n",
    "$$\\mathcal{L}(q(z);D) = - D_{KL}(q||p) +  \\log{p(D)}$$\n",
    "\n",
    "By maximizing the ELBO, we obtain an approximation $q^*(z)$ to the true posterior distribution $p(z|D)$, making the computation more tractable. It suggests that the ELBO is a trade-off between the reconstruction accuracy against the complexity of the variational posterior. The KL divergence term can be interpreted as a measure of the additional information required to express the posterior relative to the prior. As it approaches zero, the posterior is fully obtainable from the prior.\n",
    "\n",
    "In conclusion, Variational Inference (VI) is a method for approximating the true posterior distribution $p(z|D)$ in complex probabilistic models by optimizing a simpler distribution $q(z)$. The method leverages the KL divergence to measure the dissimilarity between the true posterior and the approximating distribution $q(z)$. The main goal is to minimize the KL divergence, which is equivalent to maximizing the Evidence Lower Bound (ELBO). As we see, the ELBO is a function of $q(z)$ and is related to the KL divergence and the log of the evidence $\\log{p(D)}$. By maximizing the ELBO, we find a distribution $q^*(z)$ that approximates the true posterior distribution $p(z|D)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Field Approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously discussed the need to maximize the ELBO term $\\mathcal{L}(q(z);D)$ over a certain family of distributions $Q$. The family $Q$ is a set of candidate distributions, and our goal is to find the best distribution $q^*(z)$ within that family to approximate the true posterior distribution $p(z|D)$:\n",
    "\n",
    "$$q^*(z)  = \\operatorname{argmax}_{q \\in Q}(\\mathcal{L}(q(z);D))$$\n",
    "\n",
    "However, selecting an appropriate family of distributions $Q$ is not straightforward. To address this challenge, we can employ the Mean Field approach.\n",
    "\n",
    "The Mean Field approach simplifies the approximation of the true posterior distribution $p(z|D)$ by assuming that the latent variables $z$ are independent given the parameters of the variational distribution. This assumption results in a factorized form of the distribution $q(z)$. Given a set of latent variables $z = \\{z_1, z_2, \\ldots, z_n\\}$, the Mean Field Variational Family defines the family $Q$ of candidate distributions as those that can be factorized as:\n",
    "\n",
    "$$q(z) = q(z_1, z_2, \\ldots, z_n) = q_1(z_1)q_2(z_2)\\cdots q_n(z_n) = \\prod_{i=1}^{n} q_i(z_i)$$\n",
    "\n",
    "Here, $q_i(z_i)$ represents the individual variational distributions for each latent variable $z_i$. This factorization is a direct consequence of the independence assumption. By assuming independence, we can express the joint distribution of all latent variables as a product of their individual marginal distributions.\n",
    "\n",
    "Using the calculus of variations (hence the name \"variational\"), we can derive the optimal distribution $q_{j}^{{*}}$ for each of the factors $q_{j}$ that minimizes the KL divergence, as described earlier:\n",
    "\n",
    "$$q_j^*(z_j) = \\frac{e^{\\mathbb{E}_{i \\neq j} [\\ln p(D \\cap z)] }}{ \\int e^{ \\mathbb{E}_{i \\neq j} [\\ln p(D \\cap z)] }dz_j}$$\n",
    "\n",
    "By taking the logarithm of both sides of the equation, we can further simplify the expression:\n",
    "\n",
    "$$\\log{q_j^*(z_j)} = \\log{ e^{\\mathbb{E}_{i \\neq j} [\\ln p(D \\cap z)] }} - \\log{\\int e^{ \\mathbb{E}_{i \\neq j} [\\ln p(D \\cap z)] }dz_j}$$\n",
    "\n",
    "where:\n",
    "$$\\log{q_j^*(z_j)} = \\mathbb{E}_{i \\neq j} [\\ln p(D \\cap z)] + \\text{Constant}$$\n",
    "\n",
    "This equation provides a computationally tractable way to find the optimal $q_j^*(z_j)$, which is crucial for approximating the true posterior distribution $p(z|D)$.\n",
    "\n",
    "To calculate the factorized distribution $q(z) = \\prod_{i=1}^{n} q_i(z_i)$ using the optimal variational distributions $q_j^*(z_j)$, we can follow these steps:\n",
    "\n",
    "1. Initialize the variational distributions $q_i(z_i)$ for all latent variables $z_i$. These initializations can be arbitrary, but a common approach is to use Gaussian distributions with random means and variances.\n",
    "\n",
    "2. Update each variational distribution $q_j^*(z_j)$ using the derived equation:\n",
    "  $$\\log{q_j^*(z_j)} = \\mathbb{E}_{i \\neq j} [\\ln p(D \\cap z)] + \\text{Constant}$$\n",
    "\n",
    "1. Iterate through each latent variable $z_j$ and update its corresponding variational distribution $q_j^*(z_j)$ using the equation above. This process is repeated until convergence, i.e., when the variational distributions do not change significantly between iterations. Convergence can be measured using the difference in ELBO values, or by monitoring the change in the variational distributions' parameters.\n",
    "\n",
    "2. Once the algorithm has converged, you will have the optimal variational distributions $q_j^*(z_j)$ for all latent variables $z_j$. To obtain the factorized distribution $q(z)$, you can simply multiply the individual optimal variational distributions:\n",
    "\n",
    "$$q(z) = q(z_1, z_2, \\ldots, z_n) = q_1^*(z_1)q_2^*(z_2)\\cdots q_n^*(z_n) = \\prod_{i=1}^{n} q_i^*(z_i)$$\n",
    "\n",
    "Now, we have the factorized distribution $q(z)$ that approximates the true posterior distribution $p(z|D)$. Note that the mean-field variational approach may not always provide an exact approximation, but it often yields a reasonable approximation that is computationally tractable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BNN from Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By leveraging the concepts of Variational Inference (VI), Evidence Lower Bound (ELBO), and the Mean Field approach, Bayesian Neural Networks (BNNs) can approximate the posterior distribution over the network's weights and biases. This enables BNNs to provide uncertainty estimates for their predictions, which can be crucial in tasks that require a measure of confidence in the model's output. These methods work together to transform a traditional Dense Neural Network (DNN) into a Bayesian framework. \n",
    "\n",
    "Recall that in traditional neural networks, the parameters (weights and biases) are point estimates obtained through optimization, such as gradient descent. In a BNN, instead of having point estimates for the weights and biases, we place a prior distribution over these parameters. For example, we could use Gaussian priors with zero mean and some predefined variance. This allows us to treat the network parameters as random variables and compute their posterior distribution given observed data.\n",
    "\n",
    "To compute the posterior distribution of the parameters, we can use Variational Inference (VI) to approximate it with a simpler distribution. The BNN's objective is to minimize the KL-divergence between the true posterior and the approximating distribution, which is equivalent to maximizing the Evidence Lower Bound (ELBO). By employing the Mean Field approach, we assume that the network's parameters are independent given their variational parameters. This results in a factorized form of the distribution, which simplifies the optimization problem.\n",
    "\n",
    "The Mean Field Variational Family defines the candidate distributions as those that can be factorized as:\n",
    "\n",
    "$$q(w) = q(w_1, w_2, \\ldots, w_n) = q_1(w_1)q_2(w_2)\\cdots q_n(w_n) = \\prod_{i=1}^{n} q_i(w_i)$$\n",
    "\n",
    "Here, $w$ represents the network's parameters (weights and biases), and $q_i(w_i)$ represents the individual variational distributions for each parameter. By iteratively updating the variational distributions for each parameter, we obtain an approximation to the true posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of BNNs, we use the reparameterization trick to simplify the gradient estimation during the optimization process. Instead of directly sampling from the variational distribution, we can represent each weight as $w_i = \\mu_i + \\sigma_i \\epsilon_i$, where $\\mu_i$ is the mean, $\\sigma_i$ is the standard deviation of the distribution, and $\\epsilon_i$ is a random noise term usually sampled from a standard normal distribution, i.e., $\\epsilon_i \\sim \\mathcal{N}(0, 1)$. This enables us to sample weights from the variational distribution while keeping the noise term separate, which is useful during the optimization process.\n",
    "\n",
    "\n",
    "So, in a Bayesian Neural Network (BNN), each weight is characterized by a distribution (often a Gaussian distribution), which is parameterized by a mean and a standard deviation. These parameters are learned from the data during the training process.\n",
    "\n",
    "\n",
    "Once the algorithm has converged, we can use the approximated posterior distribution of the parameters to make predictions. The output of a BNN is given by an average of the network's outputs for different parameter samples from the approximated posterior distribution. By sampling multiple sets of parameters from the distribution, we can compute an average prediction and obtain an estimate of the prediction uncertainty or draw a distribution from multiples output samples.\n",
    "\n",
    "In conclusion, BNNs can approximate the posterior distribution of the network's parameters and provide uncertainty estimates for their predictions.The posterior predictive distribution is the distribution of potential outcomes given the observed data. Each time that an input is passed through the BNN, the weights are sampled from their posterior distributions, and these weights are used to generate an output. Thus, each output can be considered a sample from the posterior predictive distribution for that input.\n",
    "\n",
    "Here I provide a step-by-step guide to implementing a Bayesian Neural Network (BNN) using Variational Inference (VI), Evidence Lower Bound (ELBO), and the Mean Field approach:\n",
    "\n",
    "1. Define the neural network architecture: Define the architecture of the neural network (number of layers, units per layer, activation functions) that you want to convert into a Bayesian Neural Network (BNN).\n",
    "\n",
    "2. Initialize the variational distributions: For each weight and bias in the neural network, initialize a variational distribution (typically a Gaussian distribution with random means and variances) that will approximate the true posterior distribution of the network's parameters.\n",
    "\n",
    "3. Define the model's likelihood: Define the likelihood function $p(D|w)$, which represents the probability of observing the data $D$ given the weights and biases $w$. In a neural network, this likelihood function is typically represented by the output layer's activation function (e.g., softmax for classification).\n",
    "\n",
    "4. Compute the ELBO: Define the Evidence Lower Bound $\\mathcal{L}(q(w);D)$ according to the formula:\n",
    "\n",
    "    $\\mathcal{L}(q(w);D) = \\mathbb{E}_{w' \\sim q}\\bigg[\\log{\\frac{p(D \\cap w')}{q(w')}}\\bigg]$\n",
    "\n",
    "    Here, $q(w')$ is the variational distribution approximating the true posterior distribution of the network's parameters (weights and biases).\n",
    "\n",
    "\n",
    "5. Optimize the variational distributions: Use an optimization algorithm (e.g., stochastic gradient descent) to update the parameters of the variational distributions by maximizing the ELBO. This step involves iterating through each parameter in the neural network and updating its corresponding variational distribution using the mean field approach. Calculate the optimal variational distributions $q_j^*(w_j)$ with the equation:\n",
    "   \n",
    "    $\\log{q_j^*(w_j)} = \\mathbb{E}_{i \\neq j} [\\ln p(D \\cap w)] + \\text{Constant}$\n",
    "\n",
    "    such that $q^*(w) = \\prod_{i=1}^{n} q_i^*(w_i) = \\operatorname{argmax}_{{q_1,\\cdots,q_n} \\in Q}(\\mathcal{L}(q(w);D))$.\n",
    "\n",
    "    Here, $q^*(w)$ represents the optimal variational distribution for the network's weights, which is a product of the individual optimal variational distributions $q_i^*(w_i)$ for each weight $w_i$. The $\\operatorname{argmax}$ notation indicates that we are looking for the set of variational distributions ${q_1,\\cdots,q_n}$ within the family $Q$ that maximizes the ELBO. In other words, We want to find the individual variational distributions for each weight that, when combined, maximize the ELBO\n",
    "\n",
    "6. Monitor convergence: Continue updating the variational distributions until the algorithm converges, i.e., when the variational distributions' parameters or the ELBO values do not change significantly between iterations. \n",
    "\n",
    "\n",
    "7. Make predictions with uncertainty estimates: Use the optimized variational distributions to make predictions with the BNN. Sample the network's parameters from the optimized variational distributions and perform a forward pass through the network. Repeat this process multiple times to obtain a distribution of predictions, which will provide an estimate of the uncertainty associated with the BNN's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but the reparameterized version explicitly demonstrates how the random variables are generated, and it may have some computational advantages in certain contexts, such as allowing gradients to flow more directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseVariational(Layer):\n",
    "    def __init__(self, units, activation=None, kl_weight=1e-3, name = 'DenseVariational_rt'):\n",
    "        \"\"\"\n",
    "        Initialize the custom variational dense layer parameters.\n",
    "        \"\"\"\n",
    "        super(DenseVariational, self).__init__(name = name)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "    def xavier(self, shape):\n",
    "        \"\"\"\n",
    "        Xavier initialization for weights and biases, ensuring proper scaling at the start of training.\n",
    "        \"\"\"\n",
    "        return tf.random.truncated_normal(\n",
    "            shape=shape,\n",
    "            mean=0.0,\n",
    "            stddev=np.sqrt(2.0 / sum(shape)))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Build the layer by defining the variables for means and standard deviations.\n",
    "        Variational Inference: The initialization of mean and standard deviation for weights and biases forms \n",
    "        the basis for approximating their posterior distributions using Gaussian distributions. \n",
    "        Mean Field: By treating weights and biases as independent random variables with their separate mean and \n",
    "        standard deviation, this approach assumes a mean-field approximation to the true joint posterior distribution.\n",
    "        \"\"\"\n",
    "        d_in = input_shape[-1]\n",
    "        # Initializing mean and standard deviation for weights and biases to represent their posterior distributions.\n",
    "        self.w_mean = tf.Variable(self.xavier((d_in, self.units)), name='w_mean')\n",
    "        self.w_std = tf.Variable(self.xavier((d_in, self.units)) - 6, name='w_std')\n",
    "        self.b_mean = tf.Variable(self.xavier((self.units,)), name='b_mean')\n",
    "        self.b_std = tf.Variable(self.xavier((self.units,)) - 6, name='b_std')\n",
    "\n",
    "    def call(self, x, training = True):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the layer, either stochastically, if in training mode, or deterministically.\n",
    "        Variational Inference: Stochastic forward pass involves sampling from approximate posterior distributions.\n",
    "        \"\"\"\n",
    "        if training: # Stochastic forward pass (sampling process)\n",
    "            \n",
    "            # Softplus to ensure std is positive\n",
    "            w_std = tf.nn.softplus(self.w_std)\n",
    "            b_std = tf.nn.softplus(self.b_std)\n",
    "            \n",
    "            # Random noise for sampling from Gaussian distributions\n",
    "            # Correlation problem: Unique perturbations for each data point\n",
    "            w_noise = tf.random.normal(shape=self.w_mean.shape)\n",
    "            b_noise = tf.random.normal(shape=self.b_mean.shape)\n",
    "            \n",
    "            # Samples from the posterior distributions for weights and biases\n",
    "            w_sample = self.w_mean + w_std*w_noise\n",
    "            b_sample = self.b_mean + b_std*b_noise\n",
    "            output = x @ w_sample + b_sample\n",
    "\n",
    "        else: # Deterministic forward pass\n",
    "            output =  x @ self.w_mean + self.b_mean\n",
    "        \n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "            \n",
    "        return output\n",
    "\n",
    "    def kl_divergence(self, mean_1, std_1, mean_2, std_2):\n",
    "        \"\"\"\n",
    "        Calculate the Kullback-Leibler (KL) divergence between two Gaussian distributions.\n",
    "        KL Divergence: The method implements the mathematical expression for KL divergence.\n",
    "        \"\"\"\n",
    "        term1 = tf.math.log(std_2 / std_1)\n",
    "        term2 = (std_1**2 + (mean_1 - mean_2)**2) / (2 * std_2**2)\n",
    "        kl = term1 + term2 - 0.5\n",
    "        return tf.reduce_sum(kl)\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        \"\"\"\n",
    "        Compute the loss for the layer, considering the Kullback-Leibler (KL) divergence \n",
    "        between the distributions of the weights and biases and a standard normal distribution.\n",
    "        KL Divergence: KL divergence is used to calculate the loss.\n",
    "        ELBO: The loss represents part of the Evidence Lower Bound.\n",
    "        \"\"\"\n",
    "\n",
    "        # Prior distribution\n",
    "        prior_mean = 0.0 \n",
    "        prior_std = 1.0\n",
    "\n",
    "        w_std = tf.nn.softplus(self.w_std)\n",
    "        b_std = tf.nn.softplus(self.b_std)\n",
    "        kl_w = self.kl_divergence(self.w_mean, w_std, prior_mean, prior_std)\n",
    "        kl_b = self.kl_divergence(self.b_mean, b_std, prior_mean, prior_std)\n",
    "        # kl of the bnn with a weight\n",
    "        kl_loss = self.kl_weight*(kl_w + kl_b)\n",
    "        return kl_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNN With flipout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In standard Bayesian Neural Networks (BNNs), the same random perturbation is often applied to all the data points within a mini-batch. This means that the sampled weights that are applied to each data point in the mini-batch are the same. This shared randomness leads to a correlation between the gradients computed for different data points in the mini-batch.\n",
    "\n",
    "This correlation within the mini-batch does not, however, extend to different mini-batches. Each new mini-batch will have its own random perturbations applied to the weights, keeping them uncorrelated with other mini-batches. The challenge with this correlation is specifically confined to individual mini-batches.\n",
    "\n",
    "Flipout is a technique designed to tackle this problem. By introducing unique random perturbations for each data point within the mini-batch, i.e. an efficient way to perturb the weights quasi-independently within a mini-batch. Flipout ensures that the gradients are decorrelated across the data points. This decorrelation helps to reduce the variance in the gradient estimates, thereby stabilizing the training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare the Data and Define Hyperparameters\n",
    "\n",
    "First, you'll need to prepare the inputs, weights, and hyperparameters. Depending on your specific problem and network architecture, these will vary.\n",
    "\n",
    "### Step 2: Define Random Sign Matrices\n",
    "\n",
    "You'll need to define the random sign matrices $ R \\) and \\( S \\), as described. These matrices have the same dimensions as your mini-batch size, and the entries are sampled uniformly from \\(\\{-1, 1\\}\\).\n",
    "\n",
    "### Step 3: Define the Flipout Operation\n",
    "\n",
    "Next, you'll need to define the operation for applying the Flipout estimator to a layer of your network. Using the equations provided:\n",
    "\n",
    "$\n",
    "Y = \\phi \\left( XW + \\left( (X \\circ S) \\hat{\\Delta W} \\right) \\circ R \\right)\n",
    "$\n",
    "\n",
    "You can define this operation as a function:\n",
    "\n",
    "\n",
    "### Step 4: Incorporate Flipout into Your Network\n",
    "\n",
    "Now, you'll need to incorporate this Flipout operation into the architecture of your network. This might involve modifying existing layers or defining new custom layers that use the `flipout_layer` function.\n",
    "\n",
    "### Step 5: Training and Backpropagation\n",
    "\n",
    "Training the model with Flipout involves typical training loop steps, including forward and backward passes. Because the Flipout operation is differentiable, you can use standard optimization algorithms like Adam or SGD.\n",
    "\n",
    "The full implementation details might vary based on your specific model architecture, problem domain, and the deep learning framework you are using. The provided explanation and code snippets should provide a starting point for integrating the Flipout estimator into your Bayesian Neural Network.\n",
    "\n",
    "Note that if you are using TensorFlow Probability, there are built-in layers like `tfp.layers.DenseFlipout` that implement this functionality, so you might not need to manually implement it unless you have specific customization requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Below, I'll describe the mathematical equations corresponding to the code provided. These equations represent the Flipout estimator and detail how the weights and perturbations are handled.\n",
    "\n",
    "1. **Sampling from the weight distribution**:\n",
    "   The weight and bias terms are sampled from their respective distributions:\n",
    "   \\[\n",
    "   \\begin{align*}\n",
    "   \\mathbf{w} &\\sim \\mathcal{N}(\\mathbf{w}_{\\text{mean}}, \\text{softplus}(\\mathbf{w}_{\\text{std}})) \\\\\n",
    "   \\mathbf{b} &\\sim \\mathcal{N}(\\mathbf{b}_{\\text{mean}}, \\text{softplus}(\\mathbf{b}_{\\text{std}}))\n",
    "   \\end{align*}\n",
    "   \\]\n",
    "\n",
    "2. **Random sign matrices (Rademacher distribution)**:\n",
    "   \\[\n",
    "   \\begin{align*}\n",
    "   \\mathbf{R} &\\sim \\text{Rademacher}(\\text{shape}=[d_{\\text{in}}, \\text{units}]) \\\\\n",
    "   \\mathbf{S} &\\sim \\text{Rademacher}(\\text{shape}=[d_{\\text{in}}, \\text{units}])\n",
    "   \\end{align*}\n",
    "   \\]\n",
    "   where \\(\\text{Rademacher}\\) denotes the Rademacher distribution, and \\(d_{\\text{in}}\\) and \\(\\text{units}\\) correspond to the dimensions of the input and the number of units in the dense layer, respectively.\n",
    "\n",
    "3. **Base perturbation**:\n",
    "   A base perturbation is drawn from a standard normal distribution:\n",
    "   \\[\n",
    "   \\hat{\\Delta \\mathbf{W}} \\sim \\mathcal{N}(0, 1)\n",
    "   \\]\n",
    "\n",
    "4. **Application of Flipout**:\n",
    "   The Flipout estimator is applied to incorporate the perturbation into the weights:\n",
    "   \\[\n",
    "   \\text{perturbation} = \\left( \\mathbf{x} \\odot \\mathbf{S} \\right) \\mathbf{\\hat{\\Delta W}} \\odot \\mathbf{R}\n",
    "   \\]\n",
    "   where \\(\\odot\\) denotes element-wise multiplication.\n",
    "\n",
    "5. **Output calculation**:\n",
    "   The output of the layer is computed by combining the sampled weights, perturbations, and biases, and applying the activation function (ReLU in this case):\n",
    "   \\[\n",
    "   \\text{output} = \\text{ReLU}\\left( \\mathbf{x}\\mathbf{w} + \\text{perturbation} + \\mathbf{b} \\right)\n",
    "   \\]\n",
    "\n",
    "These equations provide a mathematical representation of the Flipout technique as implemented in the given code, describing the stochastic behavior and variance reduction benefits of Flipout in Bayesian Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0743c798041383927c03f3bae5eca888e94e4777f716cdd511cdff96fbdedc02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
